<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" class="gr__xh-liu_github_io"><head>
<meta name="keywords" content="Xiaoqing Ye, EE, CAS, 叶晓青">
<meta name="description" content="Personal page of Xiaoqing Ye">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Xiaoqing Ye(叶晓青)'s Homepage </title>
</head>
<body data-gr-c-s-loaded="true">
<table summary="Table for page layout." id="tlayout">
<tbody><tr valign="top">
<td id="layout-menu">
<div class="menu-category">Contents</div>
<div class="menu-item"><a href="https://github.com/shuluoshu/shuluoshu.github.io/index.html" class="current">Home</a></div>
<div class="menu-item"><a href="">Resume</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Xiaoqing Ye (叶晓青) </h1>
</div>
<table class="imgtable"><tbody><tr><td>
<img src="homepage1.jpg" alt="Xiaoqing Ye" width="300px" height="270px">&nbsp;</td>
<td align="left"><p>Ph.D<br>
<a href="https://vis.baidu.com/#/">Department of Computer Vision Technology (VIS)</a> <br>
<a href="https://home.baidu.com/">Baidu Inc.</a><br><br>

<b>Email:</b><br>
yxq@whu.edu.cn <br>
yexiaoqing@baidu.com <br> <br>

<b>Interests:</b><br>
Autonomous Driving, 3D Vision, Multi-modality Large Model<br>
<br>
<a href="https://github.com/shuluoshu">Github</a>&nbsp;&nbsp;&nbsp;<a href="https://scholar.google.com/citations?user=bmN_nycAAAAJ"><b>Google Scholar</b></a></p>
</td></tr></tbody></table>
<h2>About me</h2>
<ul>
  </li>
<li><p>I am currently working in Shanghai as a senior RD at Department of Computer Vision Technology (<a href="https://vis.baidu.com/#/">VIS</a>), Baidu Inc., focusing on the Autonomous Driving since 2019/07.</p>
  </li>
    <li><p>I obtained my Ph.D. degree from the <a href="https://english.ucas.ac.cn/"> Chinese Academy of Sciences (CAS)</a> in 2019 and studied in <a href="https://en.ustc.edu.cn/"> University of Science and Technology of China (USTC)</a> during my master's program in 2015.</p>
  </li>
  <li><p>I received my B.E degree from <a href="https://www.whu.edu.cn/"> Wuhan University (WHU)</a> in 2014.</p>
  </li>
	

<br>
    My <b>research interest</b> is Autonomous Driving and 3D vision, including End-to-End AD, learning-based Planning, Perception (including multi-modal 3D object detection, polyline lane detection and occupancy network). 
<br>
    <b>Please send me an email</b> if you are interested in internship or full-time opportunities.</p>
</ul>


<h2>News</h2>
<ul>
  </li>
    <li><p>[2024]  4 papers accepted to ECCV 2024!</p>
  </li>
    <li><p>[2024]  1 paper accepted to CVPR 2024.</p>
  </li>
    <li><p>[2024]  1 paper accepted to Pattern Recognition.</p>
  </li>
    <li><p>[2024]  1 paper accepted to AAAI 2024.</p>
  </li>
	<li><p>[2024]  1 paper accepted to SCIENCE CHINA Information Sciences (SCIS) 2024 (CCF-A).</p>
  </li>
    <li><p>[2023]  1 paper accepted to NeurIPS 2023.</p>
  </li>
    <li><p>[2023]  3 papers accepted to ICCV 2023 and one paper received an <b>Oral</b>.</p>
  </li>
    <li><p>[2023]  4 Papers accepted to CVPR 2023.</p>
  </li>
    <li><p>[2023]  1 Paper accepted to ICRA 2023 as <b>Oral</b>.</p>
  </li>
    <li><p>[2023]  1 Paper accepted to AAAI 2023.</p>
  </li>
    <li><p>[2022]  1 Paper accepted to ACCV 2022 as <b>Oral</b>.</p>
  </li>
    <li><p>[2022]  1 Paper accepted to ECCV 2022.</p>
  </li>
    <li><p>[2022]  Won two <b>Champions</b> both in the CVPR 2022 SoccerNet Challenge (Pitch Localization Track and Camera Calibration Track).</p>
  </li>
  <li><p>[2022]  2 Papers accepted to ACM MM 2022.</p>
  </li>
  <li><p>[2022]  1 Paper accepted to KDD 2022 as <b>Oral</b>.</p>
  </li>
  <li><p>[2022]  2 Papers accepted to CVPR 2022.</p>
  </li>
  <li><p>[2022]  Won the <b>Champion</b> in CVPR AI City Challenge 2022 Track 1 (Multi-Camera Multi-Target Tracking).</p>
  </li>
  <li><p>[2022]  1 Paper accepted to AAAI 2022.</p>
  </li>
  <li><p>[2021]  2 Papers accepted to ICCV 2021.</p>
  </li>
  <li><p>[2021]  1 paper accepted to TPAMI 2021.</p>
  </li>
  <li><p>[2021]  4 Papers accepted to ACM MM 2021.</p>
  </li>
  <li><p>[2021]  1 Paper accepted to CVPR 2021.</p>
  </li>
  <li><p>[2021]  Won the <b>Champion</b> in ROD Challenge of ICMR 2021.</p>
  </li>
  <li><p>[2020]  1 Paper accepted to ECCV 2020. </p>
  </li>
  <li><p>[2020] 1 Paper accepted to CVPR 2020.</p>
  </li>
  <li><p>[2020] 1 Paper accepted to AAAI 2020 as <b>Oral</b>.</p>
  </li>
  <li><p>[2020] Won the <b>Champion</b> in CVPR AI City Challenge 2020 Track 1 (Multi-Class Multi-Movement Vehicle Counting).</p>
  </li>
  <li><p>[2020] 1 Paper accepted to IROS 2020.</p>
  </li>
  <li><p>[2019] Won the <b>Champion</b> in CVPR AI City Challenge 2019 Track 2 (City-Scale Multi-Camera Vehicle Re-Identification).</p>
  </li>
  <li><p>[2019] 1 Paper accepted to ICCV 2019.</p>
  </li>
  <li><p>[2018] 1 Paper accepted to ECCV 2018.</p>
  </li>
</ul>
<h2>Publications</h2>
<ul>
<p>*Equal contribution, †Corresponding author</p>


        <table class="imgtable"><tbody><tr><td>
  <img src="figs/DrivingDiffusion.png" alt="DrivingDiffusion" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2310.07771.pdf">DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model</a></b> <br>
  Xiaofan Li, Yifu Zhang, <b>Xiaoqing Ye†</b><br>
  <i>European Conference on Computer Vision (ECCV) 2024 </i><br>
  <a href="https://arxiv.org/pdf/2310.07771.pdf">PDF</a>&nbsp;&nbsp;
   <a href="https://drivingdiffusion.github.io/">Project</a>&nbsp;&nbsp;
     <a href="https://github.com/shalfun/DrivingDiffusion">Code</a>
  </td></tr></tbody></table>


  <table class="imgtable"><tbody><tr><td>
  <img src="figs/SEED.png" alt="SEED" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2407.10749"> SEED: A Simple and Effective 3D DETR in Point Clouds</a></b> <br>
  Zhe Liu, Jinghua Hou, <b>Xiaoqing Ye</b>, Tong Wang, Jingdong Wang, Xiang Bai<br>
  <i>European Conference on Computer Vision (ECCV) 2024 </i><br>
  <a href="https://arxiv.org/pdf/2407.10749">PDF</a>&nbsp;&nbsp;
   <a href="https://github.com/happinesslz/SEED">Project</a>&nbsp;&nbsp;
     <a href="https://github.com/happinesslz/SEED">Code</a>
  </td></tr></tbody></table>



  <table class="imgtable"><tbody><tr><td>
  <img src="figs/OPEN.png" alt="OPEN" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2407.10753">  OPEN: Object-wise Position Embedding for Multi-view 3D Object Detection</a></b> <br>
  Jinghua Hou, Tong Wang, <b>Xiaoqing Ye</b>, Zhe Liu, Shi Gong, Xiao Tan, Errui Ding, Jingdong Wang, Xiang Bai<br>
  <i>European Conference on Computer Vision (ECCV) 2024 </i><br>
  <a href="https://arxiv.org/pdf/2407.10753">PDF</a>&nbsp;&nbsp;
   <a href="https://github.com/AlmoonYsl/OPEN">Project</a>&nbsp;&nbsp;
     <a href="https://github.com/AlmoonYsl/OPEN">Code</a>
  </td></tr></tbody></table>



      <table class="imgtable"><tbody><tr><td>
  <img src="figs/BEVWorld.png" alt="BEVWorld" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2407.05679">BEVWorld: A Multimodal World Model for Autonomous Driving via Unified BEV Latent Space</a></b> <br>
  Yumeng Zhang, Shi Gong, Kaixin Xiong, <b>Xiaoqing Ye†</b>, Xiao Tan, Fan Wang, Jizhou Huang†, Hua Wu, Haifeng Wang<br>
  <i>Arxiv 2024 </i><br>
  <a href="https://arxiv.org/pdf/2407.05679">PDF</a>&nbsp;&nbsp;
     <a href="https://github.com/zympsyche/BevWorld">Code</a>
  </td></tr></tbody></table>


      <table class="imgtable"><tbody><tr><td>
  <img src="figs/Causality.png" alt="Causality" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2407.06546">Exploring the Causality of End-to-End Autonomous Driving</a></b> <br>
  Jiankun Li, Hao Li, Jiangjiang Liu, Zhikang Zou, <b>Xiaoqing Ye†</b>, Fan Wang, Jizhou Huang†, Hua Wu, Haifeng Wang<br>
  <i>Arxiv 2024 </i><br>
  <a href="https://arxiv.org/pdf/2407.06546">PDF</a>&nbsp;&nbsp;
     <a href="https://github.com/bdvisl/DriveInsight">Code</a>
  </td></tr></tbody></table>



        <table class="imgtable"><tbody><tr><td>
  <img src="figs/Self-Ensembling.png" alt="Self-Ensembling" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://www.zdzheng.xyz/files/PR_SEED.pdf">Self-Ensembling Depth Completion via Density-aware Consistency</a></b> <br>
  Xuanmeng Zhang, Zhedong Zheng, Minyue Jiang, <b>Xiaoqing Ye</b><br>
  <i>Pattern Recognition, 2024 </i><br>
  <a href="https://www.zdzheng.xyz/files/PR_SEED.pdf">PDF</a>&nbsp;&nbsp;
  </td></tr></tbody></table>



    <table class="imgtable"><tbody><tr><td>
  <img src="figs/CVPR_BEVspread.png" alt="CVPR_BEVspread" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_BEVSpread_Spread_Voxel_Pooling_for_Birds-Eye-View_Representation_in_Vision-based_Roadside_CVPR_2024_paper.pdf">BEVSpread: Spread Voxel Pooling for Bird’s-Eye-View Representation in Vision-based Roadside 3D Object Detection</a></b> <br>
Wenjie Wang, Yehao Lu, Guangcong Zheng, Shuigen Zhan, <b>Xiaoqing Ye</b>, Zichang Tan, Jingdong Wang, Gaoang Wang, Xi Li<br>
  <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2024</i><br>
  <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_BEVSpread_Spread_Voxel_Pooling_for_Birds-Eye-View_Representation_in_Vision-based_Roadside_CVPR_2024_paper.pdf">PDF</a>&nbsp;&nbsp;
  <a href="https://github.com/DaTongjie/BEVSpread">Code</a>
  </td></tr></tbody></table>



      <table class="imgtable"><tbody><tr><td>
  <img src="figs/SOOD_v2.png" alt="SOOD_v2" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2407.01016">SOOD++: Leveraging Unlabeled Data to Boost Oriented Object Detection</a></b> <br>
  Dingkang Liang, Wei Hua, Chunsheng Shi, Zhikang Zou, <b>Xiaoqing Ye</b>, Xiang Bai<br>
  <i>Arxiv 2024 </i><br>
  <a href="https://arxiv.org/pdf/2407.01016">PDF</a>&nbsp;&nbsp;
     <a href="">Code</a>
  </td></tr></tbody></table>

  
  <table class="imgtable"><tbody><tr><td>
  <img src="figs/CLIP-GS.png" alt="CLIP-GS" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2404.14249">CLIP-GS: CLIP-Informed Gaussian Splatting for Real-time and View-consistent 3D Semantic Understanding</a></b> <br>
  Guibiao Liao, Jiankun Li, Zhenyu Bao, <b>Xiaoqing Ye</b>, Jingdong Wang, Qing Li, Kanglin Liu<br>
  <i>Arxiv 2024 </i><br>
  <a href="https://arxiv.org/pdf/2404.14249">PDF</a>&nbsp;&nbsp;
  </td></tr></tbody></table>





    <table class="imgtable"><tbody><tr><td>
  <img src="figs/AAAI_VLM2Scene.png" alt="AAAI_VLM2Scene" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://ojs.aaai.org/index.php/AAAI/article/view/28121/28246">VLM2Scene: Self-Supervised Image-Text-LiDAR Learning with Foundation Models for Autonomous Driving Scene Understanding</a></b> <br>
Guibiao Liao, Jiankun Li, <b>Xiaoqing Ye</b>† <br>
  <i>AAAI Conference on Artificial Intelligence (AAAI) 2024</i><br>
  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28121/28246">PDF</a>&nbsp;&nbsp;
  </td></tr></tbody></table>



    <table class="imgtable"><tbody><tr><td>
  <img src="figs/pointmamba.png" alt="pointmamba" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2402.10739.pdf">PointMamba: A Simple State Space Model for
Point Cloud Analysis</a></b> <br>
Dingkang Liang, Xin Zhou, Xinyu Wang, Xingkui Zhu, Wei Xu, Zhikang Zou, <b>Xiaoqing Ye</b>, Xiang Bai <br>
  <i>Arxiv 2024</i><br>
  <a href="https://arxiv.org/pdf/2402.10739.pdf">PDF</a>&nbsp;&nbsp;
  <a href="https://github.com/LMD0311/PointMamba">Code</a>
  </td></tr></tbody></table>




    <table class="imgtable"><tbody><tr><td>
  <img src="figs/SAM3d.png" alt="SAM3d" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2306.02245.pdf">SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model</a></b> <br>
Dingyuan Zhang, Dingkang Liang, Hongcheng Yang, Zhikang Zou, <b>Xiaoqing Ye</b>, Zhe Liu, Xiang Bai <br>
  <i>SCIENCE CHINA Information Sciences (SCIS) 2024.</i><br>
  <a href="https://arxiv.org/pdf/2306.02245.pdf">PDF</a>&nbsp;&nbsp;
  <a href="https://github.com/DYZhang09/SAM3D">Code</a>
  </td></tr></tbody></table>


    <table class="imgtable"><tbody><tr><td>
  <img src="figs/ForwardFlowDNeRF.png" alt="ForwardFlowDNeRF" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2309.17390.pdf">Forward Flow for Novel View Synthesis of Dynamic Scenes </a></b> <br>
Xiang Guo, Jiadai Sun, Yuchao Dai, Guanying Chen, <b>Xiaoqing Ye</b>, Xiao Tan, Errui Ding, Yumeng Zhang, Jingdong Wang <br>
  <i>International Conference on Computer Vision (ICCV) 2023 <b>(Oral Presentation)</b> </i><br>
  <a href="https://arxiv.org/pdf/2309.17390.pdf">PDF</a>&nbsp;&nbsp;
  <a href="https://npucvr.github.io/ForwardFlowDNeRF/">Project</a>&nbsp;&nbsp
  </td></tr></tbody></table>


    <table class="imgtable"><tbody><tr><td>
  <img src="figs/AD_MLP.png" alt="AD_MLP" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2305.10430.pdf">Rethinking the Open-Loop Evaluation of End-to-End Autonomous Driving in nuScenes </a></b> <br>
Jiang-Tian Zhai, Ze Feng, Jinhao Du, Yongqiang Mao, Jiang-Jiang Liu, Zichang Tan, Yifu Zhang, <b>Xiaoqing Ye</b>, Jingdong Wang <br>
  <i>Technical Report 2023</i><br>
  <a href="https://arxiv.org/pdf/2305.10430.pdf">PDF</a>&nbsp;&nbsp;
  <a href="https://github.com/E2E-AD/AD-MLP">Code</a>&nbsp;&nbsp
  </td></tr></tbody></table>


    <table class="imgtable"><tbody><tr><td>
  <img src="figs/ISTNet.png" alt="ISTNet" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_IST-Net_Prior-Free_Category-Level_Pose_Estimation_with_Implicit_Space_Transformation_ICCV_2023_paper.pdf">IST-Net: Prior-free Category-level Pose Estimation with Implicit Space Transformation</a></b> <br>
Jianhui Liu, Yukang Chen, <b>Xiaoqing Ye</b>, Xiaojuan Qi <br>
  <i>International Conference on Computer Vision (ICCV) 2023</i><br>
  <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_IST-Net_Prior-Free_Category-Level_Pose_Estimation_with_Implicit_Space_Transformation_ICCV_2023_paper.pdf">PDF</a>&nbsp;&nbsp;
  <a href="https://github.com/CVMI-Lab/IST-Net">Code</a>&nbsp;&nbsp
  </td></tr></tbody></table>


    <table class="imgtable"><tbody><tr><td>
  <img src="figs/WSS3D.png" alt="WSS3D" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_A_Simple_Vision_Transformer_for_Weakly_Semi-supervised_3D_Object_Detection_ICCV_2023_paper.pdf">A Simple Vision Transformer for Weakly Semi-supervised 3D Object Detection</a></b> <br>
  Dingyuan Zhang, Dingkang Liang, Zhikang Zou, Jingyu Li, <b>Xiaoqing Ye</b>, Zhe Liu, Xiao Tan, Xiang Bai <br>
  <i>International Conference on Computer Vision (ICCV) 2023 </i><br>
  <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_A_Simple_Vision_Transformer_for_Weakly_Semi-supervised_3D_Object_Detection_ICCV_2023_paper.pdf">PDF</a>&nbsp;&nbsp;
  </td></tr></tbody></table>


        <table class="imgtable"><tbody><tr><td>
  <img src="figs/Diffusion.png" alt="Diffusion" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2309.02049.pdf">Diffusion-based 3D Object Detection with Random Boxes </a></b> <br>
  Xin Zhou, Jinghua Hou, Tingting Yao, Dingkang Liang, Zhe Liu, Zhikang Zou, <b>Xiaoqing Ye</b>, Jianwei Cheng, Xiang Bai <br>
  <i>Arxiv 2023</i><br>
  <a href="https://arxiv.org/pdf/2309.02049.pdf">PDF</a>&nbsp;&nbsp;
  </td></tr></tbody></table>





        <table class="imgtable"><tbody><tr><td>
  <img src="figs/FBMNet.png" alt="FBMNet" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2305.07713.pdf">Multi-Modal 3D Object Detection by Box Matching</a></b> <br>
  Zhe Liu, <b>Xiaoqing Ye</b>, Zhikang Zou, Xinwei He, Xiao Tan, Errui Ding, Jingdong Wang, Xiang Bai <br>
  <i>Arxiv 2023 </i><br>
  <a href="https://arxiv.org/pdf/2305.07713.pdf">PDF</a>&nbsp;&nbsp;
     <a href="https://github.com/happinesslz/FBMNet">Code</a>
  </td></tr></tbody></table>



    <table class="imgtable"><tbody><tr><td>
  <img src="figs/CAPE.png" alt="CAPE" width="180px" ></td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2303.10209.pdf">CAPE: Camera View Position Embedding for Multi-View 3D Object Detection</a></b> <br>
 Kaixin Xiong, Shi Gong, <b>Xiaoqing Ye</b>*, Xiao Tan, Ji Wan, Errui Ding, Jingdong Wang, Xiang Bai <br>
  <i> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023.</i><br>
  <a href="https://arxiv.org/pdf/2303.10209.pdf">PDF</a>&nbsp;&nbsp;
  <a href="https://github.com/kaixinbear/CAPE">Code</a>&nbsp;&nbsp
  </td></tr></tbody></table>


   <table class="imgtable"><tbody><tr><td>
  <img src="figs/CrowdCLIP.png" alt="CrowdCLIP" width="180px" ></td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liang_CrowdCLIP_Unsupervised_Crowd_Counting_via_Vision-Language_Model_CVPR_2023_paper.pdf">CrowdCLIP: Unsupervised Crowd Counting via Vision-Language Model</a></b> <br>
 Dingkang Liang, Jiahao Xie, Zhikang Zou, <b>Xiaoqing Ye</b>, Wei Xu, Xiang Bai <br>
  <i> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023</i><br>
  <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liang_CrowdCLIP_Unsupervised_Crowd_Counting_via_Vision-Language_Model_CVPR_2023_paper.pdf">PDF</a>&nbsp;&nbsp;
  <a href="https://github.com/dk-liang/CrowdCLIP">Code</a>&nbsp;&nbsp
  </td></tr></tbody></table>
  


  <table class="imgtable"><tbody><tr><td>
  <img src="figs/SOOD.png" alt="SOOD" width="180px" ></td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hua_SOOD_Towards_Semi-Supervised_Oriented_Object_Detection_CVPR_2023_paper.pdf">SOOD: Towards Semi-Supervised Oriented Object Detection</a></b> <br>
 Wei Hua, Dingkang Liang, Jingyu Li, Xiaolong Liu, Zhikang Zou, <b>Xiaoqing Ye</b>, Xiang Bai <br>
 <i> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023</i><br>
  <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hua_SOOD_Towards_Semi-Supervised_Oriented_Object_Detection_CVPR_2023_paper.pdf">PDF</a>&nbsp;&nbsp;
  <a href="https://github.com/HamPerdredes/SOOD">Code</a>&nbsp;&nbsp
  </td></tr></tbody></table>



  <table class="imgtable"><tbody><tr><td>
  <img src="figs/CommandDriven.png" alt="CommandDriven" width="180px" ></td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chu_Command-Driven_Articulated_Object_Understanding_and_Manipulation_CVPR_2023_paper.pdf">Command-driven Articulated Object Understanding and Manipulation</a></b> <br>
 Ruihang Chu, Zhengzhe Liu, <b>Xiaoqing Ye</b>, Xiao Tan, Xiaojuan Qi, Chi-Wing Fu, Jiaya Jia<br>
 <i> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023</i><br>
  <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chu_Command-Driven_Articulated_Object_Understanding_and_Manipulation_CVPR_2023_paper.pdf">PDF</a>
  </td></tr></tbody></table>



    <table class="imgtable"><tbody><tr><td>
  <img src="figs/QTnet.png" alt="QTnet" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://openreview.net/pdf?id=gySmwdmVDF">Query-based Temporal Fusion with Explicit Motion for 3D Object Detection</a></b> <br>
  Jinghua Hou, Zhe Liu, dingkang liang, Zhikang Zou, <b>Xiaoqing Ye</b>, Xiang Bai <br>
  <i>Neural Information Processing Systems (NeurIPS) 2023 </i><br>
  <a href="https://openreview.net/pdf?id=gySmwdmVDF">PDF</a>&nbsp;&nbsp;
  <a href="https://github.com/AlmoonYsl/QTNet">Code</a>
  </td></tr></tbody></table>


    <table class="imgtable"><tbody><tr><td>
  <img src="figs/Detach.png" alt="Detach" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2210.03952.pdf">Detaching and Boosting: Dual Engine for Scale-Invariant Self-Supervised Monocular Depth Estimation</a></b> <br>
  Peizhe Jiang, Wei Yang, <b>Xiaoqing Ye</b>, Xiao Tan, Meng Wu <br>
  <i>IEEE International Conference on Robotics and Automation (ICRA) 2023 <b>(Oral Presentation)</b> </i><br>
  <a href="https://arxiv.org/pdf/2210.03952.pdf">PDF</a>&nbsp;&nbsp;
  <a href="https://github.com/AttackonMuggle/DaB_NET0">Code</a>
  </td></tr></tbody></table>


    <table class="imgtable"><tbody><tr><td>
  <img src="figs/StereoDistill.png" alt="StereoDistill" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2301.01615.pdf">StereoDistill: Pick the Cream from LiDAR for Distilling Stereo-based 3D Object Detection</a></b> <br>
Zhe Liu, <b>Xiaoqing Ye</b>, Xiao Tan, Errui Ding, Xiang Bai <br>
  <i>AAAI Conference on Artificial Intelligence (AAAI) 2023 </i><br>
  <a href="https://arxiv.org/pdf/2301.01615.pdf">PDF</a>&nbsp;&nbsp;
  </td></tr></tbody></table>




 <table class="imgtable"><tbody><tr><td>
  <img src="figs/BytetrackV2.png" alt="BytetrackV2" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2303.15334.pdf">ByteTrackV2: 2D and 3D Multi-Object Tracking by Associating Every Detection Box</a></b> <br>  
  Yifu Zhang, Xinggang Wang, <b>Xiaoqing Ye</b>, Wei Zhang, Jincheng Lu, Xiao Tan, Errui Ding, Peize Sun, Jingdong Wang <br>
  <i> Arxiv 2023</i><br>
  <a href="https://arxiv.org/pdf/2303.15334.pdf">PDF</a>&nbsp;&nbsp;
  <a href="https://github.com/ifzhang/ByteTrack-V2">Project</a>&nbsp;&nbsp;
  <a href="https://github.com/ifzhang/ByteTrack-V2">Code</a>
  </td></tr></tbody></table>


 <table class="imgtable"><tbody><tr><td>
  <img src="figs/Rope3D.png" alt="Rope3D" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ye_Rope3D_The_Roadside_Perception_Dataset_for_Autonomous_Driving_and_Monocular_CVPR_2022_paper.pdf">Rope3D: The Roadside Perception Dataset for Autonomous Driving and Monocular 3D Object Detection Task</a></b> <br>
 <b>Xiaoqing Ye</b>*, Mao Shu*, Hanyu Li, Yifeng Shi, Yingying Li, Guangjie Wang, Xiao Tan, Errui Ding  <br>
  <i> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2022</i><br>
  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ye_Rope3D_The_Roadside_Perception_Dataset_for_Autonomous_Driving_and_Monocular_CVPR_2022_paper.pdf">PDF</a>&nbsp;&nbsp;
  <a href="https://thudair.baai.ac.cn/rope">Project</a>&nbsp;&nbsp;
  <a href="https://github.com/liyingying0113/rope3d-dataset-tools">Code</a>
  </td></tr></tbody></table>


 <table class="imgtable"><tbody><tr><td>
  <img src="figs/TWIST.png" alt="TWIST" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chu_TWIST_Two-Way_Inter-Label_Self-Training_for_Semi-Supervised_3D_Instance_Segmentation_CVPR_2022_paper.pdf">TWIST: Two-Way Inter-label Self-Training for Semi-supervised 3D Instance Segmentation. CVPR 2022.</a></b> <br>
 Ruihang Chu, <b>Xiaoqing Ye</b>, Zhengzhe Liu, Xiao Tan, Xiaojuan Qi, Chi-Wing Fu, Jiaya Jia <br>
  <i> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2022</i><br>
  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chu_TWIST_Two-Way_Inter-Label_Self-Training_for_Semi-Supervised_3D_Instance_Segmentation_CVPR_2022_paper.pdf">PDF</a>&nbsp;&nbsp;
  </td></tr></tbody></table>


    <table class="imgtable"><tbody><tr><td>
  <img src="figs/SPS.png" alt="SPS" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2209.14201.pdf">Spatial Pruned Sparse Convolution for Efficient 3D Object Detection</a></b> <br>
  Jianhui Liu, Yukang Chen, <b>Xiaoqing Ye</b>, Zhuotao Tian, Xiao Tan, Xiaojuan Qi <br>
  <i>Neural Information Processing Systems (NeurIPS) 2022</i><br>
  <a href="https://arxiv.org/pdf/2209.14201.pdf">PDF</a>&nbsp;&nbsp;
  <a href="https://github.com/CVMI-Lab/SPS-Conv">Code</a>
  </td></tr></tbody></table>



    <table class="imgtable"><tbody><tr><td>
  <img src="figs/Gitnet.png" alt="Gitnet" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2204.07733.pdf">GitNet: Geometric Prior-based Transformation for Birds-Eye-View Segmentation</a></b> <br>
        Shi Gong*, <b>Xiaoqing Ye</b>*, Xiao Tan, Jingdong Wang, Errui Ding, Yu Zhou, Xiang Bai <br>
  <i>European Conference on Computer Vision (ECCV) 2022</i><br>
  <a href="https://arxiv.org/pdf/2204.07733.pdf">PDF</a>&nbsp;&nbsp;
  <a href="">Code</a>
  </td></tr></tbody></table>


    <table class="imgtable"><tbody><tr><td>
  <img src="figs/Devilface.png" alt="Devilface" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://www.sciencedirect.com/science/article/abs/pii/S0925231222002193">The Devil is in the Face: Exploiting Harmonious Representations for Facial Expression Recognition</a></b> <br>
        Jiayi Han, Liang Du, <b>Xiaoqing Ye</b>, Li Zhang, Jianfeng Feng <br>
  <i>Neuralcomputing 2022</i><br>
  <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231222002193">PDF</a>
  </td></tr></tbody></table>



    <table class="imgtable"><tbody><tr><td>
  <img src="figs/Paint.png" alt="Paint" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2207.05497.pdf">Paint and Distill: Boosting 3D Object Detection with Semantic Passing Network</a></b> <br>
        Bo Ju, Zhikang Zou, <b>Xiaoqing Ye</b>†, Minyue Jiang, Xiao Tan, Errui Ding, Jingdong Wang<br>
  <i>ACM International Conference on Multimedia (MM) 2022</i><br>
  <a href="https://arxiv.org/pdf/2207.05497.pdf">PDF</a>
  </td></tr></tbody></table>



    <table class="imgtable"><tbody><tr><td>
  <img src="figs/NDVG.png" alt="NDVG" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://openaccess.thecvf.com/content/ACCV2022/papers/Guo_Neural_Deformable_Voxel_Grid_for_Fast_Optimization_of_Dynamic_View_ACCV_2022_paper.pdf">Paint and Distill: Boosting 3D Object Detection with Semantic Passing Network</a></b> <br>
        Xiang Guo, Guanying Chen, Yuchao Dai, <b>Xiaoqing Ye</b>, Jiadai Sun, Xiao Tan, Errui Ding<br>
  <i>Asian Conference on Computer Vision (ACCV) 2022 <b>(Oral Presentation)</b> </i><br>
  <a href="https://openaccess.thecvf.com/content/ACCV2022/papers/Guo_Neural_Deformable_Voxel_Grid_for_Fast_Optimization_of_Dynamic_View_ACCV_2022_paper.pdf">PDF</a>&nbsp;&nbsp;
  <a href="https://npucvr.github.io/NDVG">Project</a>&nbsp;&nbsp;
  <a href="https://github.com/SeanGuo063/NDVG">Code</a>
  </td></tr></tbody></table>


    <table class="imgtable"><tbody><tr><td>
  <img src="figs/KDD.png" alt="KDD" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://dl.acm.org/doi/abs/10.1145/3534678.3539029">DuARE: Automatic Road Extraction with Aerial Images and Trajectory Data at Baidu Maps</a></b> <br>
        Jianzhong Yang*, <b>Xiaoqing Ye</b>*, Bin Wu, Yanlei Gu, Ziyu Wang, Deguo Xia, Jizhou Huang<br>
  <i>ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) 2022 <b>(Oral Presentation)</b> </i><br>
  <a href="https://dl.acm.org/doi/abs/10.1145/3534678.3539029">PDF</a>
  </td></tr></tbody></table>


    <table class="imgtable"><tbody><tr><td>
  <img src="figs/SGM3D.png" alt="SGM3D" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2112.01914.pdf">SGM3D: Stereo Guided Monocular 3D Object Detection</a></b> <br>
        Zheyuan Zhou*, Liang Du*, <b>Xiaoqing Ye</b>*, Zhikang Zou, Xiao Tan, Li Zhang, Xiangyang Xue, Jianfeng Feng<br>
  <i>IEEE Robotics and Automation Letters (RA-L) 2022</i><br>
  <a href="https://arxiv.org/pdf/2112.01914.pdf">PDF</a>&nbsp;&nbsp;
  <a href="https://github.com/zhouzheyuan/sgm3d">Project</a>
  </td></tr></tbody></table>

   
      <table class="imgtable"><tbody><tr><td>
  <img src="figs/hanAAAI.png" alt="hanAAAI" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://ojs.aaai.org/index.php/AAAI/article/view/19962/19721">Modify the self-attention via skeleton decomposition for effective point-cloud transformer</a></b> <br>
        Jiayi Han, Longbin Zeng, Liang Du, <b>Xiaoqing Ye</b>, Weiyang Ding, Jianfeng Feng<br>
  <i>AAAI Conference on Artificial Intelligence (AAAI) 2022</i><br>
  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/19962/19721">PDF</a>
  </td></tr></tbody></table>
  
  

  <table class="imgtable"><tbody><tr><td>
  <img src="figs/AGOnet.png" alt="AGOnet" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2208.11658.pdf">AGO-Net: Association-Guided 3D Point Cloud Object Detection Network</a></b> <br>
        Liang Du, <b>Xiaoqing Ye</b>, Xiao Tan, Edward Johns, Bo Chen, Errui Ding, Xiangyang Xue, Jianfeng Feng<br>
  <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2021</i><br>
  <a href="https://arxiv.org/pdf/2208.11658.pdf">PDF</a>
  </td></tr></tbody></table>


         <table class="imgtable"><tbody><tr><td>
  <img src="figs/Box-Grained.png" alt="Box-Grained" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Yang_Box-Grained_Reranking_Matching_for_Multi-Camera_Multi-Target_Tracking_CVPRW_2022_paper.pdf">Box-Grained Reranking Matching for Multi-Camera Multi-Target Tracking</a></b> <br>
        Xipeng Yang, Jin Ye, Jincheng Lu, Chenting Gong, Minyue Jiang, Xiangru Lin, Wei Zhang, Xiao Tan, Yingying Li, <b>Xiaoqing Ye</b>, Errui Ding <br>
  <i> IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRw) 2022 </i><br>
  <a href="https://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Yang_Box-Grained_Reranking_Matching_for_Multi-Camera_Multi-Target_Tracking_CVPRW_2022_paper.pdf">PDF</a>&nbsp;&nbsp;
  <a href="https://github.com/Yejin0111/AICITY2022-Track1-MTMC">Code</a>
  </td></tr></tbody></table>


    <table class="imgtable"><tbody><tr><td>
  <img src="figs/DFR.png" alt="DFR" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zou_The_Devil_Is_in_the_Task_Exploiting_Reciprocal_Appearance-Localization_Features_ICCV_2021_paper.pdf">The Devil is in the Task: Exploiting Reciprocal Appearance-Localization Features for Monocular 3D Object Detection</a></b> <br>
        Zhikang Zou*, <b>Xiaoqing Ye</b>*, Liang Du*, Xianhui Cheng*, Xiao Tan, Li Zhang, Jianfeng Feng, Xiangyang Xue, Errui Ding<br>
  <i>International Conference on Computer Vision (ICCV) 2021 </i><br>
  <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zou_The_Devil_Is_in_the_Task_Exploiting_Reciprocal_Appearance-Localization_Features_ICCV_2021_paper.pdf">PDF</a>
  </td></tr></tbody></table>


    <table class="imgtable"><tbody><tr><td>
  <img src="figs/OA_ICCV.png" alt="OA_ICCV" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Revealing_the_Reciprocal_Relations_Between_Self-Supervised_Stereo_and_Monocular_Depth_ICCV_2021_paper.pdf">Revealing the Reciprocal Relations between Self-Supervised Stereo and Monocular Depth Estimation</a></b> <br>
        Zhi Chen, <b>Xiaoqing Ye</b>, Zhikang Zou, Wei Yang, Zhenbo Xu, Xiao Tan, Zhikang Zou, Errui Ding, Xinming Zhang, Liusheng Huang<br>
  <i>International Conference on Computer Vision (ICCV) 2021 </i><br>
  <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Revealing_the_Reciprocal_Relations_Between_Self-Supervised_Stereo_and_Monocular_Depth_ICCV_2021_paper.pdf">PDF</a>
  </td></tr></tbody></table>
  

    <table class="imgtable"><tbody><tr><td>
  <img src="figs/Depth-conditioned.png" alt="Depth-conditioned" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Depth-Conditioned_Dynamic_Message_Propagation_for_Monocular_3D_Object_Detection_CVPR_2021_paper.pdf">Depth-conditioned Dynamic Message Propagation for Monocular 3D Object Detection</a></b> <br>
        Li Wang*, Liang Du*, <b>Xiaoqing Ye</b>*, Yanwei Fu, Guodong Guo, Xiangyang Xue, Jianfeng Feng, Li Zhang<br>
  <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2021</i><br>
  <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Depth-Conditioned_Dynamic_Message_Propagation_for_Monocular_3D_Object_Detection_CVPR_2021_paper.pdf">PDF</a>&nbsp;&nbsp;
  <a href="https://github.com/fudan-zvg/DDMP">Project</a>
  </td></tr></tbody></table>



    <table class="imgtable"><tbody><tr><td>
  <img src="figs/AggNet.png" alt="AggNet" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://dl.acm.org/doi/abs/10.1145/3474085.3475287">AggNet for Self-supervised Monocular Depth Estimation: Go An Aggressive Step Further </a></b> <br>
        Zhi Chen*, <b>Xiaoqing Ye</b>*, Liang Du, Wei Yang, Liusheng Huang, Xiao Tan, Zhenbo Shi, Fumin Shen, Errui Ding<br>
  <i>ACM International Conference on Multimedia (MM) 2021</i><br>
  <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475287">PDF</a>
  </td></tr></tbody></table>



    <table class="imgtable"><tbody><tr><td>
  <img src="figs/Lift.png" alt="Lift" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://dl.acm.org/doi/10.1145/3474085.3475277">Lifting the Veil of Frequency in Joint Segmentation and Depth Estimation</a></b> <br>
        Tianhao Fu*, Yingying Li*, <b>Xiaoqing Ye</b>*, Xiao Tan, Hao Sun, Fumin Shen, Errui Ding<br>
  <i>ACM International Conference on Multimedia (MM) 2021</i><br>
  <a href="https://dl.acm.org/doi/10.1145/3474085.3475277">PDF</a>
  </td></tr></tbody></table>



  <table class="imgtable"><tbody><tr><td>
  <img src="figs/LiftMM.png" alt="LiftMM" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2107.12858.pdf">Coarse to Fine: Domain Adaptive Crowd Counting via Adversarial Scoring Network</a></b> <br>
        Zhikang Zou, Xiaoye Qu, Pan Zhou, Shuangjie Xu, <b>Xiaoqing Ye</b>, Wenhao Wu, Jin Ye<br>
  <i>ACM International Conference on Multimedia (MM) 2021</i><br>
  <a href="https://arxiv.org/pdf/2107.12858.pdf">PDF</a>
  </td></tr></tbody></table>
  

  <table class="imgtable"><tbody><tr><td>
  <img src="figs/CountingMM.png" alt="CountingMM" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2104.10868.pdf">Towards Adversarial Patch Analysis and Certified Defense against Crowd Counting</a></b> <br>
        Qiming Wu, Zhikang Zou, Pan Zhou, <b>Xiaoqing Ye</b>, Binghui Wang, Ang Li<br>
  <i>ACM International Conference on Multimedia (MM) 2021</i><br>
  <a href="https://arxiv.org/pdf/2104.10868.pdf">PDF</a>
  </td></tr></tbody></table>




  <table class="imgtable"><tbody><tr><td>
  <img src="figs/DAnet.png" alt="DAnet" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://dl.acm.org/doi/pdf/10.1145/3460426.3463656">Towards Adversarial Patch Analysis and Certified Defense against Crowd Counting</a></b> <br>
        Bo Ju, Wei Yang, Jinrang Jia, <b>Xiaoqing Ye</b>, Qu Chen, Xiao Tan, Hao Sun, Yifeng Shi, Errui Ding<br>
  <i>International Conference on Multimedia Retrieval (ICMR) 2021</i><br>
  <a href="https://dl.acm.org/doi/pdf/10.1145/3460426.3463656">PDF</a>
  </td></tr></tbody></table>



   <table class="imgtable"><tbody><tr><td>
  <img src="figs/DA3ddet.png" alt="DA3ddet" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540018.pdf">Monocular 3d Object Detection via Feature Domain Adaptation</a></b> <br>
        <b>Xiaoqing Ye</b>*, Liang Du*, Yifeng Shi, Yingying Li, Xiao Tan, Jianfeng Feng, Errui Ding, Shilei Wen <br>
  <i>European Conference on Computer Vision (ECCV) 2020</i><br>
  <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540018.pdf">PDF</a>
  </td></tr></tbody></table>
  

   <table class="imgtable"><tbody><tr><td>
  <img src="figs/Zoomnet.png" alt="Zoomnet" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://arxiv.org/pdf/2003.00529.pdf">Part-aware Adaptive Zooming Neural Network for 3d Object Detection</a></b> <br>
        Zhenbo Xu, Wei Zhang, <b>Xiaoqing Ye</b>, Xiao Tan, Wei Yang, Shilei Wen, Errui Ding, Ajin Meng, Liusheng Huang <br>
  <i>AAAI Conference on Artificial Intelligence (AAAI) 2020 <b>(Oral Presentation)</b> </i><br>
  <a href="https://arxiv.org/pdf/2003.00529.pdf">PDF</a>&nbsp;&nbsp;
  <a href="https://github.com/detectRecog/ZoomNet">Code</a>
  </td></tr></tbody></table>


     <table class="imgtable"><tbody><tr><td>
  <img src="figs/Associate.png" alt="Associate" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Du_Associate-3Ddet_Perceptual-to-Conceptual_Association_for_3D_Point_Cloud_Object_Detection_CVPR_2020_paper.pdf">Associate-3Ddet: Perceptual-to-conceptual association for 3D point cloud object detection</a></b> <br>
        Liang Du*, <b>Xiaoqing Ye</b>*, Xiao Tan, Jianfeng Feng, Zhenbo Xu, Errui Ding, Shilei Wen <br>
  <i> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2020 </i><br>
  <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Du_Associate-3Ddet_Perceptual-to-Conceptual_Association_for_3D_Point_Cloud_Object_Detection_CVPR_2020_paper.pdf">PDF</a>&nbsp;&nbsp;
  <a href="">Code</a>
  </td></tr></tbody></table>


     <table class="imgtable"><tbody><tr><td>
  <img src="figs/RegionNet.png" alt="RegionNet" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="http://ras.papercept.net/images/temp/IROS/files/0144.pdf">RegionNet: Region-feature-enhanced 3D Scene Understanding Network with Dual Spatial-aware Discriminative Loss</a></b> <br>
        Guanghui Zhang, Dongchen Zhu, <b>Xiaoqing Ye</b>, Wenjun Shi, Minghong Chen, Jiamao Li, Xiaolin Zhang <br>
  <i> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2020 </i><br>
  <a href="http://ras.papercept.net/images/temp/IROS/files/0144.pdf">PDF</a>
  </td></tr></tbody></table>
 

     <table class="imgtable"><tbody><tr><td>
  <img src="figs/Leapnet.png" alt="Leapnet" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://link.springer.com/chapter/10.1007/978-3-030-66096-3_47">Leaping from 2D Detection to Efficient 6DoF Object Pose Estimation </a></b> <br>
        Jinhui Liu*, Zhikang Zou*, <b>Xiaoqing Ye</b>*, Xiao Tan, Errui Ding, Feng Xu, Xin Yu <br>
  <i> European Conference on Computer Vision Workshops (ECCVw) 2020 </i><br>
  <a href="https://link.springer.com/chapter/10.1007/978-3-030-66096-3_47">PDF</a>
  </td></tr></tbody></table>



     <table class="imgtable"><tbody><tr><td>
  <img src="figs/Vehiclecounting.png" alt="Vehiclecounting" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w35/Liu_Robust_Movement-Specific_Vehicle_Counting_at_Crowded_Intersections_CVPRW_2020_paper.pdf">Robust Movement-specific Vehicle Counting at Crowded Intersections</a></b> <br>
        Zhongji Liu, Wei Zhang, Xu Gao, Hao Meng, Xiao Tan, Xiaoxing Zhu, Zhan Xue, <b>Xiaoqing Ye</b>, Hongwu Zhang, Shilei Wen, Errui Ding <br>
  <i> IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRw) 2020 </i><br>
  <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w35/Liu_Robust_Movement-Specific_Vehicle_Counting_at_Crowded_Intersections_CVPRW_2020_paper.pdf">PDF</a>&nbsp;&nbsp;
  <a href="">Code</a>
  </td></tr></tbody></table>
  

       <table class="imgtable"><tbody><tr><td>
  <img src="figs/Multicamera.png" alt="Multicamera" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/AI%20City/Tan_Multi-camera_vehicle_tracking_and_re-identification_based_on_visual_and_spatial-temporal_CVPRW_2019_paper.pdf">Multi-camera vehicle tracking and re-identification based on visual and spatial-temporal features</a></b> <br>
        Xiao Tan, Zhigang Wang, Minyue Jiang, Xipeng Yang, Jian Wang, Yuan Gao, Xiangbo Su, <b>Xiaoqing Ye</b>, Yuchen Yuan, Dongliang He, Shilei Wen, Errui Ding <br>
  <i> IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRw) 2019 </i><br>
  <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/AI%20City/Tan_Multi-camera_vehicle_tracking_and_re-identification_based_on_visual_and_spatial-temporal_CVPRW_2019_paper.pdf">PDF</a>&nbsp;&nbsp;
  <a href="">Code</a>
  </td></tr></tbody></table>




       <table class="imgtable"><tbody><tr><td>
  <img src="figs/SSF.png" alt="SSF" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Du_SSF-DAN_Separated_Semantic_Feature_Based_Domain_Adaptation_Network_for_Semantic_ICCV_2019_paper.pdf">SSF-DAN: Separated Semantic Feature based Domain Adaptation Network for Semantic Segmentation</a></b> <br>
        Liang Du, Jingang Tan, Hongye Yang, Jianfeng Feng, Xiangyang Xue, Qibao Zheng, <b>Xiaoqing Ye</b>, Xiaolin Zhang <br>
  <i> IEEE International Conference on Computer Vision (ICCV) 2019 </i><br>
  <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Du_SSF-DAN_Separated_Semantic_Feature_Based_Domain_Adaptation_Network_for_Semantic_ICCV_2019_paper.pdf">PDF</a>&nbsp;&nbsp;
  <a href="https://github.com/Biotan/SSF-DAN">Code</a>
  </td></tr></tbody></table>

  


     <table class="imgtable"><tbody><tr><td>
  <img src="figs/Recurrent.png" alt="Recurrent" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Xiaoqing_Ye_3D_Recurrent_Neural_ECCV_2018_paper.pdf">3D Recurrent Neural Networks with Context Fusion for Point Cloud Semantic Segmentation</a></b> <br>
        <b>Xiaoqing Ye</b>, Jiamao Li, Hexiao Huang, Liang Du, Xiaolin Zhang <br>
  <i> European Conference on Computer Vision (ECCV) 2018 </i><br>
  <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Xiaoqing_Ye_3D_Recurrent_Neural_ECCV_2018_paper.pdf">PDF</a>
  </td></tr></tbody></table>

  


     <table class="imgtable"><tbody><tr><td>
  <img src="figs/Order.png" alt="Order" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://ieeexplore.ieee.org/document/8010259">Order-Based Disparity Refinement Including Occlusion Handling for Stereo Matching</a></b> <br>
        <b>Xiaoqing Ye</b>, Yuzhang Gu, Lili Chen, Jiamao Li, Han Wang, Xiaolin Zhang <br>
  <i>IEEE Signal Processing Letters (IEEE SP-L) 2017 </i><br>
  <a href="https://ieeexplore.ieee.org/document/8010259">PDF</a>
  </td></tr></tbody></table>

  

     <table class="imgtable"><tbody><tr><td>
  <img src="figs/FEN.png" alt="FEN" width="180px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        <a  href="https://www.jstage.jst.go.jp/article/transinf/E100.D/12/E100.D_2017EDL8122/_pdf">Feature Ensemble Network with Occlusion Disambiguation for Accurate Patch-Based Stereo Matching</a></b> <br>
        <b>Xiaoqing Ye</b>, Jiamao Li, Han Wang, Xiaolin Zhang <br>
  <i>IEICE TRANSACTIONS on Information and Systems 2017 </i><br>
  <a href="https://www.jstage.jst.go.jp/article/transinf/E100.D/12/E100.D_2017EDL8122/_pdf">PDF</a>
  </td></tr></tbody></table>




</ul>
    <h2>Professional Activities</h2>
<ul>
</li>
<li> Journal Reviewer of TPAMI, IJCV, TCSVT etc.
</li>
<li> Conference Reviewer of CVPR, ECCV, ICCV, AAAI, NeurIPS, etc., and serving as a Senior Program Committee (SPC) member of AAAI.
</li>
</ul>


</ul>
    <h2>Part of the Work I Participated in</h2>
<ul>
</li>
<li> <a href="https://www.youtube.com/watch?v=8xxZxQVfU_A">[Video] UniBEV: Road-Vehicle Cooperative Perception for Autonomous Driving | Baidu Create 2022</a>
</li>
<li> <a href="https://www.apollo.auto/apolloday/">[Video] 文心大模型在自动驾驶感知的落地应用 & 汽车智能化时代的L4/L2+技术共生 | Baidu Apollo Day 2022 </a>
</li>
<li> <a href="https://thudair.baai.ac.cn/rope">[V2X 3D Object Detection Dataset Download] </a>  &  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ye_Rope3D_The_Roadside_Perception_Dataset_for_Autonomous_Driving_and_Monocular_CVPR_2022_paper.pdf">[Related Paper] Rope3D: The Roadside Perception Dataset for Autonomous Driving and Monocular 3D Object Detection Task </a>
</li>

</ul>


<h2>Other Interests</h2>
<table class="imgtable"><tbody><tr><td>
  <img src="wx.png" alt="wx" width="240px" >&nbsp;</td>
      <td align="left"><b><style="font-size:100%">
        I also have a personal WeChat Official Account, to keep a record of life and share some moments and experiences, if you are interested, do not hesitate to scan the code to follow me.
        <br>
  </td></tr></tbody></table>
  

</body></html>
